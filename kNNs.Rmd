---
title: "Nearest Neighbor Search"
author: "David Oliver"
date: "2/4/2020"
output: html_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(babynames)
library(plotly)
library(magrittr)
library(conflicted)
library(kableExtra)
#library(class)
conflict_prefer("layout", "plotly")

```

# Nearest Neighbor Search -- Framing the Question

Given a set $S$ of points in a space $M$ and a query point $q \in M$, find the closest
point in $S$ to $q$. Most commonly, $M$ is taken to be the d-dimensional vector space
where dissimilarity is measured using some distance metric.

## Basic Scenario

Let's start by defining a set of points, $S$, in a 2-dimensional space, $M$. 

```{r, Initial Data}

S <- babynames$name %>% unique() %>% sample(100, replace = F) %>% 
     data.frame(Names = ., X = rnorm(100), Y = rnorm(100))

plot_ly(S, x = ~X, y = ~Y, 
        type = "scatter", mode = "markers", 
        name = "babies", hoverinfo = "text", text = ~Names,
        color = 1, colors = c("#1a9850")) %>% 
    hide_colorbar()

```

Now define a new point, $q$, also in 2-dimensions.  

```{r}

q <- data.frame(Names = "q", X = rnorm(1), Y = rnorm(1)) 

S <- rbind.data.frame(S, q) %>% add_column(., source = c(rep("OLD", 100), "NEW"))

plot_ly(S, x = ~X, y = ~Y, type = "scatter", mode = "markers",
        hoverinfo = "text", text = ~Names, 
        color = ~source, colors = c("#d73027", "#1a9850")) %>% 
    layout(showlegend = FALSE)

```

That's it, we're all set up. Now we want to find the nearest neighbor to the point $q$. 

Determining the nearest neighbor means that we need to find some distance, or more
generally a dissimilarity, metric on which to judge nearness.

The choice of metrics is dependant on your problem and there is no clear method for making
the decision other than rationale relating to the specifics of the data and the question.

In the mean time, if you are desparate for a place to start and all the options have given
you analysis paralysis, try 
[GUSTA ME](https://sites.google.com/site/mb3gustame/wizards/-dis-similarity-wizard)'s 
(Dis)similarity wizard as a useful guide for where to start.

That being said, for this example, there really isn't any reason to get fancy with the 
metric yet, so we will stick with euclidean distance. The Euclidean distance in 2D space 
between any two points $p$ and $q$ is $d=\sqrt{(x_{p}-x_{q})^2+(y_{p}-y_{q})^2}$

```{r}

# if p and q are two n dimensionsal points, then the euclidean distance is
euclidist <- function(p, q){
    stopifnot(length(p) == length(q))
    sqrt(sum((p - q)^2))
}

S$euclid_dist <- apply(S[,2:3], 1, euclidist, q = q[,2:3])
S$euclid_dist[S$euclid_dist == 0] <- NA

S[which.min(S$euclid_dist),]

plot_ly(S, x = ~X, y = ~Y, type = "scatter", mode = "markers",
        hoverinfo = "text", text = ~Names,
        color = ~source, colors = c("#d73027", "#1a9850")) %>% 
    add_segments(., x = ~X, xend = q$X, y = ~Y, yend = q$Y,
                 line = list(color="#000000", width=0.5, dash="solid")) %>% 
    layout(showlegend = FALSE)

```

The nearest neighbor to $q \in M$ from set $S$ is `r min(S$euclid_dist, na.rm = T)` units away and
is named `r S$Names[which.min(S$euclid_dist)]`.

# Nearest-Neighbors Search -- All Pairwise Neighbors

An interesting property of nearest-neighbors (really it is a property of distance 
metrics) is that a given point $p$ may have nearest neighbor $q$, but $p$ is not 
necessarily $q$'s nearest neighbor. 

At the moment I'm not sure what the usefulness of this property is at the moment, but
let's try to find the nearest neighbor for each point in our dataset. In order to gain
some computational efficiency we'll just use `dist` from the `stats` package.

```{r, fig.height=7, fig.show="animate", interval=0.05}

# calculate the distance between each pair of points
distMat <- 
    dist(S[, c("X", "Y")], method = "euclidean", upper = T) %>% 
    as.matrix
diag(distMat) <- NA

# the nearest neighbor of each point is the minimum value in each row or column of the 
# square distance matrix (with diag = NA)
minDist <- apply(distMat, 1, which.min)

# collecting the target and destination nodes
S$NNFrom <- as.numeric(names(minDist))
S$NNTo <- minDist

# get X-Y values for both origin (X0,Y0) and target (X1,Y1) nodes.
SNet <- data.frame(Name0 = S$Names, X0 = S$X, Y0 = S$Y, X1 = S$X[S$NNTo], 
                   Y1 = S$Y[S$NNTo], source = S$source)

plot_ly(SNet, x = ~X0, y = ~Y0, type = "scatter", mode = "markers",
        marker = list(size = 10), hoverinfo = "text", 
        text = ~Name0, color = ~source, colors = c("#d73027", "#1a9850")) %>% 
    add_annotations(., x = ~X1, ax = ~X0,
                    y = ~Y1, ay = ~Y0, 
                    xref = "x", yref = "y",
                    axref = "x", ayref = "y",
                    text = "", arrowhead = 2,
                    showarrow = T) %>% 
    layout(xaxis = list(title="", zeroline = F, showticklabels = F, showgrid = F), 
           yaxis = list(title="", zeroline = F, showticklabels = F, showgrid = F)) 


```

From this directed graph, we can see that nearest-neighbors are not always recipricol.

# Nearest Neighbor Search -- K Nearest-Neighbors (K-NNs)

K-NN search extends nearest-neighbors to the $k$ nearest, so instead of looking
for a single nearest neighbor we are looking for the $k$ nearest-neighbors.

K-NN can help answer different questions about the data depending on what we are 
interested in. Below are a couple of questions that k-NNs can answer.

1. Often the idea is to perform classification of a new set of observations based on
a training set of observations with known classifications. In this case, we set
$k$ equal to some number of neighbors and the classification among those neighbors
which appears most often is the classification for the new data point. 

2. Alternatively, we can ask a question about the density of points around a data point.
Using the distance to $k$th nearest-neighbors, we can estimate the local density
of data around a given point.

3. Other questions k-NN can answer...


Let's answer the first two questions in order. First we'll look at the classification
problem.

```{r}

# assign 2 different classes
S$Class <- 
    sample(c("Male", "Female"), nrow(S)-1, replace = T) %>% 
    c(., "Unknown") %>% factor

plot_ly(S, x = ~X, y = ~Y, type = "scatter", mode = "markers", color = ~Class,
        colors = c("#fb9a99", "#a6cee3", "#33a02c"), hoverinfo = "text",
        text = ~Names, marker = list(size = 8))

# First neighbor is k-NN where k = 1, we have already calculated the distances
# to q from each point. We sort the data by that distance and then the top k
# observations are the nearest-neighbors
S %<>% arrange(euclid_dist)

# the first 3 nearest-neighbors to q
S[1:3,]

```


So that is how we decide nearest-neighbors. We do need to do some math to determine which
class is most represented (classification) or the "average" class (regression). Choosing 
$k$, we can assign our new data point to the class which is most highly represented among 
the $k$ nearest-neighbors i.e. the mode. Unfortunately, R stats and base do not have a 
built in mode function. So we'll write one here which is a functionalization of 
[this solution]([https://stackoverflow.com/a/8189441/1701678).


```{r}

# since mode is already a function in R (not the mathematical mode), we'll use .mode 
.mode <- function(x, len = c("one", "all")) {
    l <- match.arg(len)
    ux <- unique(x)
    if(l == "one"){
        res <- ux[which.max(tabulate(match(x, ux)))]
    } else if(l == "all"){
        tab <- tabulate(match(x, ux))
        res <- ux[tab == max(tab)]
    } else {
        stop("Failed to select approriate argument for parameter len.")
    }
    return(res)
}

```


Now we'll write a non-optimal k nearest-neighbors finding algorithm. Since we'd like to be
able to maybe do this on the fly at some later stage, we'll writ it as a tidy-ish function 
meaning it takes a data.frame with user specified columns for distance calculations and 
classes.


```{r}

# in the simplest case we want to find the classification for a single point
# for which distances have already been calculated for our dataset. 
findKNNSingle <- function(data, dists, classes, k,
                          method = c("classification", "regression")){
    dists <- enexpr(dists)
    classes <- enexpr(classes)
    # check for proper inputs
    if(!is.data.frame(data))
        stop("data must be a data.frame")
    if(!is.numeric(pull(data, dists)))
        stop("distances must be numeric")
    if(!is.factor(pull(data, classes)))
        stop("classes must be factors")
    if(length(k) != 1)
        stop("k must be of length 1")
    method <- match.arg(method)
    # sort the data by shortest distance, and select the top k classes
    # which are the classes of the closest k points.
    S <- data %>% top_n(., k, -!!dists) %>% pull(., !!classes)
    # if we want straight classification use .mode, otherwise use the average
    if(method == "classification"){
        res <- .mode(S, len = "all")
    } else if(method == "regression"){
        classTab <- table(S, useNA = "no")
        classMeans <- classTab/sum(classTab)
        res <- names(classMeans[classMeans == max(classMeans)])
    } else {
        stop("Failed to select appropriate method.")
    }
    # break ties at random
    if(length(res) != 1){
        res <- sample(res, size = 1)
    }
    return(res)
}

```


Now that we've built a basic KNN classification algorithm, we can check the assignment of
our new datapoint $q$. 


```{r}

findKNNSingle(data = S, dists = euclid_dist, classes = Class, 
              k = 9, method = "classification") %>% 
    as.character()

# let's try multiple values for k
class_of_q <- lapply(3:11, function(i){
    findKNNSingle(data = S, dists = euclid_dist, classes = Class, 
                  k = i, method = "classification") %>% 
    as.character()
}) %>% do.call(rbind, .) %>% data.frame(k_value = 3:11, class_new = .)

kable(class_of_q) %>% kable_styling(bootstrap_options = c("striped", "hover"))

```

By checking the class assignment of $q$ using several different values for k, the majority 
of K-NN searches classified $q$ as `r table(class_of_q) %>% which.max() %>% names()`.  



# Nearest Neighbor Search -- Shared Nearest-Neighbors (S-NNs)

# Nearest Neighbor Search -- Mutual Nearest-Neighbors (M-NNs)

# Nearest Neighbor Search -- Nearest-neighbor chain algorithm

It seems that maybe this is important for nearest-neighbor chain algorithms. In addition,
this concept might be important for S-NN and M-NN approaches. 

# Nearest Neighbor Search -- Farthest Neighbors Search (FNs)






